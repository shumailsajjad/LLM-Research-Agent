{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa533b08-a7bf-46c8-bda8-e3b0e68d7004",
   "metadata": {},
   "source": [
    "# Building with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80480c1a-b9da-487f-9293-8f991ba57dbd",
   "metadata": {},
   "source": [
    "## Building a ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd85174-7bcc-4000-83ba-2874ed15c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade -q openai langchain langchain-openai langchain-community langgraph langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33823fa-9733-449f-829e-2bdf203a7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637d3655-8d9f-4ef1-a3b1-9976cf1e6696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the API key from .env\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4beecb-74ed-44ef-8e0f-bd7019677666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "\n",
    "# defining the chatbot node\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# adding the node to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# setting the entry and the finish points\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3359e9-2377-4185-b530-c361d0fb5274",
   "metadata": {},
   "source": [
    "## Visualizing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652d99be-f2e3-42cb-a9af-a85aef41fce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5566c1-429d-4030-95b2-f95652de39d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4778a311-0a74-42b1-891e-1419babfa99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | chatbot |   \n",
      " +---------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n"
     ]
    }
   ],
   "source": [
    "print(graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b64d7-ffed-4dfb-8a78-b9f98c3633ef",
   "metadata": {},
   "source": [
    "## Running the ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20697292-5540-40d6-a787-37721497da7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Paris is ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Paris is the capital city of France, known for its rich history, art, fashion, and culture. It is famous for iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The city is often referred to as \"The City of Light\" and is celebrated for its romantic ambiance, world-class cuisine, and vibrant street life. Paris is also a major center for business, education, and diplomacy in Europe. Whether you're exploring its charming neighborhoods like Montmartre or enjoying a stroll along the Seine River, Paris offers a unique blend of history and modernity.\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  History of Notre-Dame.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The history of Notre-Dame Cathedral, located on the Île de la Cité in Paris, France, is rich and complex, spanning nearly 900 years. Here is an overview of its significant milestones:\n",
      "\n",
      "### Early History and Construction\n",
      "- **12th Century**: The site of Notre-Dame has been a place of worship since antiquity, with various churches built there before the current cathedral. Construction of the present Gothic structure began in 1163 under Bishop Maurice de Sully. The cathedral was built to accommodate the growing population of Paris and to serve as a symbol of the Church’s power.\n",
      "- **Construction Phases**: The cathedral was built in several phases, with the main structure completed by 1250. The iconic flying buttresses, which support the walls and allow for large stained glass windows, were added later in the 13th century.\n",
      "\n",
      "### Architectural Features\n",
      "- **Gothic Architecture**: Notre-Dame is a prime example of French Gothic architecture, characterized by its ribbed vaults, flying buttresses, and ornate sculptures. The cathedral features numerous gargoyles and spires, as well as stunning rose windows.\n",
      "- **Interior and Art**: The interior boasts remarkable artworks, including stained glass windows, sculptures, and altarpieces. The famous rose windows, particularly the north and south ones, are among the largest and most beautiful in the world.\n",
      "\n",
      "### Historical Events\n",
      "- **Coronations and Ceremonies**: Notre-Dame has been the site of many significant events, including the coronation of Napoleon Bonaparte in 1804 and the funeral of prominent figures such as Victor Hugo and Charles de Gaulle.\n",
      "- **French Revolution**: During the French Revolution (1789-1799), the cathedral suffered damage as revolutionaries sought to dismantle symbols of the monarchy and the Church. Many religious artifacts were destroyed, and it was briefly used as a warehouse.\n",
      "- **Restoration**: In the 19th century, a major restoration project was led by architect Eugène Viollet-le-Duc, who aimed to restore the cathedral to its former glory. This included the addition of the iconic spire, which was completed in 1859.\n",
      "\n",
      "### Modern Era\n",
      "- **Cultural Significance**: Notre-Dame has remained a symbol of French culture and history, attracting millions of visitors annually. It has been featured in literature, art, and film, most notably in Victor Hugo's novel \"The Hunchback of Notre-Dame\" (1831), which helped spark interest in the cathedral's preservation.\n",
      "- **2019 Fire**: On April 15, 2019, a devastating fire broke out, causing the spire to collapse and significant damage to the roof and interior. The event shocked the world and led to an outpouring of support for restoration efforts.\n",
      "\n",
      "### Restoration and Future\n",
      "- **Reconstruction Efforts**: Following the fire, a massive restoration effort has been underway, with plans to restore the cathedral to its pre-fire state. The French government has committed significant funding, and restoration work is expected to take several years.\n",
      "- **Reopening**: As of now, plans are in place to reopen Notre-Dame to the public in 2024, in time for the Paris Olympics.\n",
      "\n",
      "Notre-Dame Cathedral stands not only as a masterpiece of architecture but also as a testament to the history and resilience of Paris and France as a whole. Its ongoing restoration is a symbol of hope and renewal for many.\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input('User: ')\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye',  'q']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    for event in graph.stream({'messages': ('user', user_input)}):\n",
    "        for value in event.values():\n",
    "            print(f'Assistant: {value[\"messages\"][-1].content}')\n",
    "            print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8e583-81c6-4b09-84f6-66d4480af33c",
   "metadata": {},
   "source": [
    "## Tavily AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ddc45e-0a80-4d9f-a9ee-eefd5d633729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4573fb0-6232-42af-9a3d-4ffcc0e214e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "362a7552-f2f6-4dfe-b031-63febaa6432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'EUFA EURO 2024 FINAL',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'UEFA Euro 2024 final - Wikipedia',\n",
       "   'url': 'https://en.wikipedia.org/wiki/UEFA_Euro_2024_Final',\n",
       "   'content': \"The UEFA Euro 2024 final was a football match that determined the winners of UEFA Euro 2024.The match was the seventeenth final of the European Championship, a quadrennial tournament contested by the men's national teams of the member associations of UEFA to decide the champions of Europe. The match was held at the Olympiastadion in Berlin, Germany, on 14 July 2024, and was contested by Spain ...\",\n",
       "   'score': 0.99942297,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Spain vs. England Highlights | UEFA Euro 2024 | Final',\n",
       "   'url': 'https://www.foxsports.com/watch/fmc-6s3o670adn0rrlmt',\n",
       "   'content': 'Check out the top moments between Spain and England in the UEFA Euro 2024 Final! JULY 14・fox soccer now・11:23. UEFA EuroEnglandSpainLamine YamalNico WilliamsCole PalmerMikel OyarzabalHarry ...',\n",
       "   'score': 0.99816847,\n",
       "   'raw_content': None},\n",
       "  {'title': 'EURO 2024 final: Who was in it? When and where was it? | UEFA EURO 2024',\n",
       "   'url': 'https://www.uefa.com/euro2024/news/0284-18bb952a9458-2a9e1ff202c4-1000--euro-2024-final-when-and-where-is-it/',\n",
       "   'content': 'The UEFA EURO 2024 final was played on Sunday 14 July, kicking off at 21:00 CET. The final: all the reaction. The match took place at Olympiastadion Berlin, the biggest stadium at the tournament ...',\n",
       "   'score': 0.9974191,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Spain vs England 2-1: UEFA Euro 2024 final - as it happened',\n",
       "   'url': 'https://www.aljazeera.com/sports/liveblog/2024/7/14/live-spain-vs-england-uefa-euro-2024-final',\n",
       "   'content': '14 Jul 2024. Spain beat England 2-1 in the final of the European Football Championships, Euro 2024. Spain named an unchanged team for the match at the Olympiastadion in Berlin, Germany while ...',\n",
       "   'score': 0.99698395,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Spain beat England to win Euro 2024 final with late Oyarzabal goal',\n",
       "   'url': 'https://supersport.com/football/uefa-euro/news/42ab7223-039f-47b9-8d28-8f652498118d/spain-beat-england-to-win-euro-2024-final-with-late-oyarzabal-goal',\n",
       "   'content': \"Substitute Mikel Oyarzabal scored a dramatic late winner as Spain triumphed in Sunday's Euro 2024 final, beating England 2-1 to confirm their re-emergence as a force on the international stage and ...\",\n",
       "   'score': 0.99670136,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 2.05}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "# initializing a Tavily client\n",
    "client = TavilyClient(api_key=os.environ.get('TAVILY_API_KEY'))\n",
    "\n",
    "response = client.search(query='EUFA EURO 2024 FINAL')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657c7f2f-a13a-4006-99e7-e042b7bf0bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: UEFA Euro 2024 final - Wikipedia, URL: https://en.wikipedia.org/wiki/UEFA_Euro_2024_Final\n",
      "Title: Spain vs. England Highlights | UEFA Euro 2024 | Final, URL: https://www.foxsports.com/watch/fmc-6s3o670adn0rrlmt\n",
      "Title: EURO 2024 final: Who was in it? When and where was it? | UEFA EURO 2024, URL: https://www.uefa.com/euro2024/news/0284-18bb952a9458-2a9e1ff202c4-1000--euro-2024-final-when-and-where-is-it/\n",
      "Title: Spain vs England 2-1: UEFA Euro 2024 final - as it happened, URL: https://www.aljazeera.com/sports/liveblog/2024/7/14/live-spain-vs-england-uefa-euro-2024-final\n",
      "Title: Spain beat England to win Euro 2024 final with late Oyarzabal goal, URL: https://supersport.com/football/uefa-euro/news/42ab7223-039f-47b9-8d28-8f652498118d/spain-beat-england-to-win-euro-2024-final-with-late-oyarzabal-goal\n"
     ]
    }
   ],
   "source": [
    "for result in response['results']:\n",
    "    print(f\"Title: {result['title']}, URL: {result['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a77b6a-2578-4bc8-b2f8-72058c6b8612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are LLM agents?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': 'LLM agents are advanced AI systems designed for creating complex text that requires sequential reasoning. They are capable of thinking ahead, remembering past conversations, and adjusting their responses based on the situation and required style. These agents can utilize tools like a planning module for question-decomposition, a RAG pipeline for information retrieval, and memory modules for accurate handling of subquestions. They play a crucial role in enterprise applications by enabling tasks such as generating context-aware answers, solving complex programmatically tasks, and searching for information over the internet using APIs.',\n",
       " 'images': ['https://promptengineering.org/content/images/2023/08/Prompt-engineering---Large-Language-Model-LLM--Autonomous-Agent-Structure---PromptEngineering.org.jpg',\n",
       "  'https://gptpluginz.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-30-at-2.11.50-PM-1024x562.jpg',\n",
       "  'https://uploads-ssl.webflow.com/62528d398a42420e66390ef9/64df7b56979c7be76bfdff36_Untitled (900 × 350 px) (1).png',\n",
       "  'https://gptpluginz.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-30-at-3.11.05-PM-1024x516.png',\n",
       "  'https://promptengineering.org/content/images/2023/07/Prompt-engineering---Large-Language-Model-LLM--Autonomous-Agent.jpg'],\n",
       " 'results': [{'title': 'LLM agents: The ultimate guide | SuperAnnotate',\n",
       "   'url': 'https://www.superannotate.com/blog/llm-agents',\n",
       "   'content': 'LLM agents are advanced AI systems designed for creating complex text that needs sequential reasoning. They can think ahead, remember past conversations, and use different tools to adjust their responses based on the situation and style needed. Consider a question in the legal field that sounds like this:',\n",
       "   'score': 0.9995102,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Introduction to LLM Agents | NVIDIA Technical Blog',\n",
       "   'url': 'https://developer.nvidia.com/blog/introduction-to-llm-agents',\n",
       "   'content': 'Demystifying Retrieval-Augmented Generation Pipelines\\nBuild Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model\\nTraining a Text2Sparql Model with MK-SQuIT and NeMo\\nRelated posts\\nDevelop Custom Enterprise Generative AI with NVIDIA NeMo\\nStreamline Evaluation of LLMs for Accuracy with NVIDIA NeMo Evaluator\\nNVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records\\nAn Easy Introduction to Multimodal Retrieval Augmented Generation\\nHow to Take a RAG Application from Pilot to Production in Four Steps While there isn’t a widely accepted definition for LLM-powered agents, they can be described as a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools.\\n To answer this question, you essentially must answer three questions individually (i.e., we need a planning module):\\nIn this case, you would need an agent that has access to a \\xa0Planning Module that does question-decomposition (generates sub-questions and searches for answers till the larger problem is solved), a RAG pipeline (used as a tool) to retrieve specific information, and memory modules to accurately handle the subquestions. Agents for enterprise applications\\nWhile the applications of agents are practically boundless, the following are a few interesting cases that may have an outsized impact for many businesses:\\n“Talk to your data” agent\\n“Talk to your data” isn’t a simple problem. For instance, agents can use a RAG pipeline to generate context aware answers, a code interpreter to solve complex programmatically tasks, an API to search information over the internet, or even any simple API service like a weather API or an API for an Instant messaging application.\\n',\n",
       "   'score': 0.9960699,\n",
       "   'raw_content': None},\n",
       "  {'title': 'LLM Agents — Intuitively and Exhaustively Explained',\n",
       "   'url': 'https://towardsdatascience.com/llm-agents-intuitively-and-exhaustively-explained-8905858e18e2',\n",
       "   'content': 'Sign up\\nSign in\\nSign up\\nSign in\\nMember-only story\\nLanguage Modeling | Autonomous Systems | Artificial Intelligence\\nLLM Agents — Intuitively and Exhaustively Explained\\nEmpowering Language Models to Reason and Act\\nDaniel Warfield\\nFollow\\nTowards Data Science\\n--\\n7\\nShare\\nThis article focuses on “Agents”, a general concept that allows language models to reason and interact with the world. Help\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams First, we’ll discuss what agents are and why they’re important, then we’ll take a look at a few forms of agents to build an intuitive understanding of how they work, then we’ll explore agents in a practical context by implementing two of them, one using LangChain and one from scratch in Python.\\n --\\n--\\n7\\nWritten by Daniel Warfield\\nTowards Data Science\\nData Scientist, Teacher, and Writer. It’s an expansion of in-context learning which allows a user to inject information retrieved from a document into a prompt, thus allowing a language model to make inferences on never before seen information.\\n',\n",
       "   'score': 0.99586606,\n",
       "   'raw_content': None},\n",
       "  {'title': 'LLM Agents | Prompt Engineering Guide',\n",
       "   'url': 'https://www.promptingguide.ai/research/llm-agents',\n",
       "   'content': \"These are the possible high-level components of the hypothetical LLM agent but there are still important considerations such as creating a plan to address the task and potential access to a memory module that helps the agent keep track of the state of the flow of operations, observations, and overall progress.\\n The figure below shows an example of ReAct and the different steps involved in performing question answering:\\nLearn more about ReAct here:\\nMemory\\nThe memory module helps to store the agent's internal logs including past thoughts, actions, and observations from the environment, including all interactions between agent and user. To better motivate the usefulness of an LLM agent, let's say that we were interested in building a system that can help answer the following question:\\nWhat's the average daily calorie intake for 2023 in the United States?\\n LLM Agent Framework\\nGenerally speaking, an LLM agent framework can consist of the following core components:\\nAgent\\nA large language model (LLM) with general-purpose capabilities serves as the main brain, agent module, or coordinator of the system. Now let's give the system a more complex question like the following:\\nHow has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates?\",\n",
       "   'score': 0.99408895,\n",
       "   'raw_content': None},\n",
       "  {'title': \"Navigating the World of LLM Agents: A Beginner's Guide\",\n",
       "   'url': 'https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9',\n",
       "   'content': 'Sign up\\nSign in\\nSign up\\nSign in\\nMember-only story\\nNavigating the World of LLM Agents: A Beginner’s Guide\\nA Step-by-Step Guide to Discover and Harness the Power of LLM Agents and Toolkits\\nDominik Polzer\\nFollow\\nTowards Data Science\\n--\\n1\\nShare\\nTable of Contents\\nIntro\\nWhat are Agents?What — The Chain-of-ThoughtThe Agent Executor — The Agent behind the Agent\\nFrom Theory to Practice\\nHow to Use the SQLDatabaseToolkit?Hands-On Tutorial\\nSummary\\nIntro\\nThis article is about how we can get LLMs (Large Language Models) to solve complex tasks independently.\\n linkedin.com/in/polzerdo/\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams --\\n--\\n1\\nWritten by Dominik Polzer\\nTowards Data Science\\nMachine Learning Engineer. This concept we want to transfer to LLMs to continuously make new decisions and thus gradually approaching the solution to more complex problems.\\n',\n",
       "   'score': 0.99262565,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Building Your First LLM Agent Application | NVIDIA Developer',\n",
       "   'url': 'https://developer.nvidia.com/blog/building-your-first-llm-agent-application/',\n",
       "   'content': \"By the end of this post, the agent you build will answer complex and layered questions like the following:\\nAs described in part 1 of this series, there are four agent components:\\nTools\\nTo build an LLM agent, you need the following tools:\\nPlanning module\\nWith this LLM agent, you will be able to answer questions such as: “How much did the revenue grow between Q1 of 2024 and Q2 of 2024?” Related resources\\nTags\\nAbout the Authors\\nComments\\nRelated posts\\nIntroduction to LLM Agents\\nGetting Started with Large Language Models for Enterprise Solutions\\nAn Introduction to Large Language Models: Prompt Engineering and P-Tuning\\nNVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems\\nNew Ebook: A Beginner's Guide to Large Language Models\\nRelated posts\\nAchieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM\\nGenerative AI Research Spotlight: Recommended reading list for building agents\\nThere are plenty of resources and materials that you can use to stimulate your thinking around what is possible with agents, but the following resources are an excellent starting point to cover the overall ethos of agents:\\nIf you are looking for more reading material, I find the Awesome LLM-Powered Agent list to be useful. Memory\\nNext, you must build a memory module to keep track of all the questions being asked or just to keep a list of all the sub-questions and the answers for said questions.\\n In the next step (Figure 5), you provide the input from the RAG pipeline that the answer wasn’t available, so the agent then decides to decompose the question into simpler sub-parts.\\n\",\n",
       "   'score': 0.9847524,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Intro to LLM Agents with Langchain: When RAG is Not Enough',\n",
       "   'url': 'https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834',\n",
       "   'content': 'Good luck with your AI projects and don’t hesitate to reach out if you need help at your company!\\n--\\n--\\nWritten by Alex Honchar\\nTowards Data Science\\nCo-founder @ Neurons Lab\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Notice, how we can easily decompose and define separately:\\nThe final definition of the agent will look as simple as this:\\nAs you can see in the outputs of the script (or you can run it yourself), it solves the issue in the previous part related to tools. For example, while building the tree of thoughts prompts, I save my sub-prompts in the prompts repository and load them:\\nYou can see in this notebook the result of such reasoning, the point I want to make here is the right process for defining your reasoning steps and versioning them in such an LLMOps system like Langsmith. If you prefer a narrative walkthrough, you can find the YouTube video here:\\nAs always, you can find the code on GitHub, and here are separate Colab Notebooks:\\nIntroduction to the agents\\nLet’s begin the lecture by exploring various examples of LLM agents. Also, you can see other examples of popular reasoning techniques in public repositories like ReAct or Self-ask with search:\\nOther notable approaches are:\\nStep 2: Memory\\nStep 3: Tools\\nChatGPT Plugins and OpenAI API function calling are good examples of LLMs augmented with tool use capability working in practice.\\n',\n",
       "   'score': 0.9777563,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 3.42}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.search(\n",
    "    query='What are LLM agents?',\n",
    "    search_depth='advanced',\n",
    "    max_results=7,\n",
    "    include_images=True,\n",
    "    include_answer=True,\n",
    "    include_raw_content=False\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac70237-229d-4c60-973f-aedbb0a1e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid won the UEFA Champions League in 2024 by defeating Borussia Dortmund 2-0 in the final on June 1st at Wembley Stadium.\n"
     ]
    }
   ],
   "source": [
    "answer = client.qna_search(query='Who won the UEFA Champions League in 2024?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b60a94c-6b99-4dc4-b5d3-2399db1b1d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Tutorials - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraphjs/tutorials/',\n",
       "  'content': 'Reflection & Critique Tutorials¶ Welcome to the LangGraph Tutorials! These notebooks introduce LangGraph through building various language agents and applications. Quick Start¶ Learn the basics of LangGraph through a comprehensive quick start in which you will build an agent from scratch.',\n",
       "  'score': 0.9991896,\n",
       "  'raw_content': None},\n",
       " {'title': 'Tutorials - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/tutorials/',\n",
       "  'content': 'Reflection & Critique¶ Basic Reflection: Prompt the agent to reflect on and revise its outputs; Reflexion: Critique missing and superfluous details to guide next steps; Language Agent Tree Search: Use reflection and rewards to drive a tree search over agents; Self-Discover Agent: Analyze an agent that learns about its own capabilities ...',\n",
       "  'score': 0.9986646,\n",
       "  'raw_content': None}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.adapters.openai import convert_openai_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = 'What is the \"Reflection & Critique\" pattern used in agentic applications and LangGraph?'\n",
    "\n",
    "response = client.search(query, max_results=5, search_depth='advanced')['results']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a7d4d1-41d1-4b3f-86a9-3f9be95cadf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI critical thinker research assistant. \\n        Your sole purpose is to write well written, objective and structured reports on given text.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Information: \"\"\"[{\\'title\\': \\'Tutorials - GitHub Pages\\', \\'url\\': \\'https://langchain-ai.github.io/langgraphjs/tutorials/\\', \\'content\\': \\'Reflection & Critique Tutorials¶ Welcome to the LangGraph Tutorials! These notebooks introduce LangGraph through building various language agents and applications. Quick Start¶ Learn the basics of LangGraph through a comprehensive quick start in which you will build an agent from scratch.\\', \\'score\\': 0.9991896, \\'raw_content\\': None}, {\\'title\\': \\'Tutorials - GitHub Pages\\', \\'url\\': \\'https://langchain-ai.github.io/langgraph/tutorials/\\', \\'content\\': \\'Reflection & Critique¶ Basic Reflection: Prompt the agent to reflect on and revise its outputs; Reflexion: Critique missing and superfluous details to guide next steps; Language Agent Tree Search: Use reflection and rewards to drive a tree search over agents; Self-Discover Agent: Analyze an agent that learns about its own capabilities ...\\', \\'score\\': 0.9986646, \\'raw_content\\': None}]\"\"\"\\n        Using the above information, answer the following query: \"\"\"What is the \"Reflection & Critique\" pattern used in agentic applications and LangGraph?\"\"\" in a detailed report', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up the OpenAI API prompt\n",
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': f'''You are an AI critical thinker research assistant. \n",
    "        Your sole purpose is to write well written, objective and structured reports on given text.'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f'''Information: \"\"\"{response}\"\"\"\n",
    "        Using the above information, answer the following query: \"\"\"{query}\"\"\" in a detailed report'''\n",
    "    }\n",
    "]\n",
    "\n",
    "lc_messages = convert_openai_messages(prompt)\n",
    "lc_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a86bda9a-4b75-4a77-ace3-ab73a8dfbb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report on the \"Reflection & Critique\" Pattern in Agentic Applications and LangGraph\n",
      "\n",
      "## Introduction\n",
      "The \"Reflection & Critique\" pattern is a significant component of agentic applications, particularly in the context of LangGraph, a framework designed for creating language agents and applications. This report explores the structure and purpose of the \"Reflection & Critique\" pattern as presented in the LangGraph tutorials.\n",
      "\n",
      "## Overview of LangGraph\n",
      "LangGraph is a framework that allows developers to build language agents capable of interacting with users and performing complex language-based tasks. The tutorials provided by LangGraph serve as a guide to understanding and implementing various functionalities of the framework, including the Reflection & Critique pattern.\n",
      "\n",
      "## Reflection & Critique Pattern\n",
      "The Reflection & Critique pattern encompasses various techniques aimed at enhancing the performance and reliability of language agents. Below are the core components of this pattern as highlighted in the tutorials:\n",
      "\n",
      "1. **Basic Reflection**: \n",
      "   - This technique involves prompting the agent to reflect on and revise its outputs. It encourages agents to assess their responses critically, identifying areas for improvement.\n",
      "\n",
      "2. **Reflexion**: \n",
      "   - Reflexion serves as a method for critiquing the agent's outputs by identifying missing or superfluous details. This step is crucial in guiding the agent toward more accurate and concise responses in future interactions.\n",
      "\n",
      "3. **Language Agent Tree Search**: \n",
      "   - This component utilizes reflection and reward mechanisms to drive a tree search over agents. It allows the agent to explore various pathways in generating responses, leading to better decision-making processes and outcomes.\n",
      "\n",
      "4. **Self-Discover Agent**: \n",
      "   - This aspect focuses on enabling the agent to analyze its own capabilities. By understanding its strengths and weaknesses, the agent can improve its performance and adapt to different tasks more effectively.\n",
      "\n",
      "## Practical Applications\n",
      "The Reflection & Critique pattern is fundamental in developing language agents that can adapt, learn, and refine their outputs over time. By incorporating these reflective practices, agents become more responsive to user inputs and capable of producing higher-quality content.\n",
      "\n",
      "## Conclusion\n",
      "The \"Reflection & Critique\" pattern is an essential methodological framework within LangGraph that fosters the development of sophisticated language agents. By emphasizing self-reflection, critique, and self-discovery, this pattern not only enhances the agents' ability to generate accurate responses but also contributes to their overall learning and adaptability. These principles are vital for creating effective and intelligent applications that can meet user needs in a dynamic environment. \n",
      "\n",
      "For further exploration of these concepts, users are encouraged to refer to the LangGraph tutorials available at [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/).\n"
     ]
    }
   ],
   "source": [
    "response = ChatOpenAI(model='gpt-4o-mini').invoke(lc_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd937c0-af90-41ba-b8af-2f935219a2ea",
   "metadata": {},
   "source": [
    "## Enhancing the Chatbot with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d44c5791-4256-4c2e-aa67-e3d514fd023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac95aef7-cd9c-4dbf-aaa1-3c54af522e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tools\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a774c189-c49d-4e1e-a3d0-905b22c1400a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.techtarget.com/WhatIs/feature/OpenAI-o1-explained-Everything-you-need-to-know',\n",
       "  'content': \"OpenAI's o1 models, launched in September 2024, enhance reasoning in AI and excel in complex tasks, such as generating and debugging code. The o1 models are initially intended to be preview models, designed to provide users -- as well as OpenAI -- with a different type of LLM experience than the GPT-4o model. OpenAI o1 can perform many tasks like any of OpenAI's other GPT models -- such as answering questions, summarizing content and generating new content. The o1 models are optimized for complex reasoning tasks, especially in STEM (science, technology, engineering and mathematics). For API users OpenAI o1 is more expensive than previous models -- including GPT-4o. ##### What businesses should know about OpenAI's GPT-4o model  By: Will Kelly\"},\n",
       " {'url': 'https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/',\n",
       "  'content': 'ChatGPT maker OpenAI has announced its next major product release: A generative AI model code-named Strawberry, officially called OpenAI o1. OpenAI o1 avoids some of the reasoning pitfalls that normally trip up generative AI models because it can effectively fact-check itself by spending more time considering all parts of a question. What makes o1 “feel” qualitatively different from other generative AI models is its ability to “think” before responding to queries, according to OpenAI. (That’s less impressive when you consider that Google DeepMind’s recent AI achieved a silver medal in an equivalent to the actual IMO contest.) OpenAI also says that o1 reached the 89th percentile of participants — better than DeepMind’s flagship system AlphaCode 2, for what it’s worth — in the online programming challenge rounds known as Codeforces.'},\n",
       " {'url': 'https://openai.com/o1/',\n",
       "  'content': \"Introducing OpenAI o1. We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and other updates. Try it in ChatGPT Plus Try it in the API.\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tool.invoke('What is GPT o1?')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a01d1629-a633-4c24-9233-5130751abcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# --- new import\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "tools = [tool]\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "\n",
    "# tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# change the chatbot() node function. Use llm_with_tools instead of llm.\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# --- add this:\n",
    "# run the tools if they are called by adding the tools to a new node.\n",
    "# This node runs the tools requested in the last AIMessage.\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "# ---\n",
    "\n",
    "# --- add this\n",
    "# define the conditional_edges.\n",
    "# we'll use the prebuilt tools_condition in the conditional_edge to route to the ToolNode if the last message has tool calls,\n",
    "# otherwise, route to the end.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# ---\n",
    "\n",
    "# any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# we don't need to explicitly set a finish_point because our graph already has a way to finish!\n",
    "# graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71c47170-d408-42ba-a590-afc0c03f306a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize the graph we've built. We'll use the same code as in the previous video.\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf3fd46a-9894-4c49-8d17-5c7323e5fc03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Who won the gold medal in 200m free style swimming at the Paris Olympics?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  \n",
      "--------------------------------------------------\n",
      "Assistant:  [{\"url\": \"https://www.reuters.com/sports/olympics/swimming-popovici-takes-mens-200-metres-freestyle-gold-2024-07-29/\", \"content\": \"Item 1 of 4 Paris 2024 Olympics - Swimming - Men's 200m Freestyle Final - Paris La Defense Arena, Nanterre, France - July 29, 2024. David Popovici of Romania looks on after winning gold.\"}, {\"url\": \"https://bleacherreport.com/articles/10130042-olympic-swimming-2024-womens-200m-freestyle-medal-winners-times-and-results\", \"content\": \"Swimming (Olympic) Olympic Swimming 2024: Women's 200M Freestyle Medal Winners, Times and Results ... the 2016 gold medalist, had on the mid-range freestyle events at the Olympics. Titmus swept ...\"}, {\"url\": \"https://olympics.com/en/paris-2024/videos/men-s-200m-freestyle-final-swimming-olympic-games-paris-2024\", \"content\": \"The Men's 200m Freestyle was held on 29/07/2024 at Paris La Defense Arena. David Popovici (ROU) claimed Gold with a time of 1:44.72. Matthew Richards (GBR) took Silver and Luke Hobson (USA) completed the podium.\"}]\n",
      "--------------------------------------------------\n",
      "Assistant:  David Popovici of Romania won the gold medal in the men's 200m freestyle swimming event at the Paris Olympics on July 29, 2024, with a time of 1:44.72. Matthew Richards from Great Britain took the silver, and Luke Hobson from the USA completed the podium.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  NVIDIA A100 vs. NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  \n",
      "--------------------------------------------------\n",
      "Assistant:  [{\"url\": \"https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686\", \"content\": \"2560x1440. 3840x2160. The RTX A6000 is an enthusiast-class professional graphics card by NVIDIA, launched on October 5th, 2020. Built on the 8 nm process, and based on the GA102 graphics processor, the card supports DirectX 12 Ultimate. The GA102 graphics processor is a large chip with a die area of 628 mm² and 28,300 million transistors.\"}, {\"url\": \"https://www.storagereview.com/review/nvidia-rtx-a6000-review\", \"content\": \"We installed the RTX A6000 in both the HP ZCentral 4R and the P620 workstations, which the RTX 8000 also used in some of our benchmarks:\\nHP ZCentral 4R:\\nLenovo ThinkStation P620:\\nWe installed the RTX 8000 and RTX 3090 in a custom consumer-build for our Blender and LuxMark tests:\\nSPECviewperf 2020\\nFirst up is the SPECviewperf 2020 benchmark, the worldwide standard for measuring graphics performance of professional applications running under the OpenGL and Direct X application programming interfaces. Yes, the RTX A6000 is a direct replacement of the RTX 8000 and technically the successor to the RTX 6000, but it is actually more in line with the RTX 3090 in many ways, as far as specifications and potential performance output go. NVIDIA RTX A6000 Specifications\\n12 GB, 16 GB, 24 GB, 48 GB\\nConnect 2x RTX A6000\\nPerformance\\nTo gauge its performance, we put the RTX A6000 through a series of resource-intensive tests and compared it to the RTX 8000 and the RTX 3090 FE. It’s incredibly hard to get a hold of any RTX card these days, but if you are in a position to do so, the RTX A6000 best suits those who work primarily in CAD (more specifically, in scientific fields with applications like Creo and CATIA), as the RTX 3090 doesn’t provide the necessary driver optimizations that come with the Quadro line of cards. In the food rendering category, the RTX A6000 scored 8,088 when inside the Lenovo P620, which more than doubled that of the RTX 8000 (which posted 3,337) and slightly trailed the RTX 3090 (which posted 8,929).\"}, {\"url\": \"https://www.cudocompute.com/blog/nvidia-rtx-a6000-everything-you-need-to-know\", \"content\": \"The A6000 is a high-performance GPU built on the NVIDIA Ampere architecture and designed to handle memory-intensive tasks across various applications. The NVIDIA RTX A6000's second-generation RT Cores offer up to twice the throughput of the first-generation RT Cores, significantly enhancing real-time ray tracing performance. The NVIDIA RTX A6000 features third-generation Tensor Cores that introduce the new Tensor Float 32 (TF32) precision, allowing up to 5X the training throughput compared to the Turing-based GPUs (such as the Quadro RTX 6000 and RTX 8000), significantly accelerating AI and data science model training without needing any code modifications. The NVIDIA A6000 GPU demonstrates significant performance enhancements for deep learning applications.\"}]\n",
      "--------------------------------------------------\n",
      "Assistant:  ### Overview of NVIDIA A100 vs. NVIDIA RTX A6000\n",
      "\n",
      "Both the NVIDIA A100 and RTX A6000 are powerful GPUs designed for different applications, with specific strengths in AI, data science, and graphics rendering.\n",
      "\n",
      "#### NVIDIA A100\n",
      "- **Architecture**: Ampere\n",
      "- **Memory**: Available in configurations of 40 GB and 80 GB of HBM2e memory.\n",
      "- **Memory Interface**: 5120-bit\n",
      "- **Boost Clock**: Up to 1410 MHz\n",
      "- **Power Consumption**: Requires an 8-pin EPS power connector.\n",
      "- **Performance**: Optimized for AI and deep learning tasks, the A100 is particularly strong in training models and performing high-throughput computations. Benchmarks indicate that it can be significantly faster than the RTX A6000 in certain AI workloads (61% faster in specific tests).\n",
      "- **Use Case**: Primarily used in data centers and for AI training, scientific computing, and high-performance computing (HPC).\n",
      "\n",
      "#### NVIDIA RTX A6000\n",
      "- **Architecture**: Ampere\n",
      "- **Memory**: 48 GB of GDDR6 memory, scalable up to 96 GB with NVLink.\n",
      "- **Performance**: Designed for professional graphics workloads, the A6000 excels in rendering, visualization, and CAD applications. It features second-generation RT Cores for enhanced ray tracing performance and third-generation Tensor Cores for improved AI workloads.\n",
      "- **Use Case**: Targeted at professionals in design, engineering, and scientific fields, making it suitable for visual computing tasks.\n",
      "\n",
      "### Key Comparisons\n",
      "- **Performance**: The A100 is generally faster in AI workloads compared to the RTX A6000, which is more focused on graphics performance.\n",
      "- **Memory Type**: A100 uses HBM2e, while A6000 uses GDDR6, which affects their respective performance in specific applications.\n",
      "- **Applications**: A100 is better suited for data science and AI training, while A6000 is optimized for graphics-intensive applications.\n",
      "\n",
      "### Additional Resources\n",
      "- For a detailed comparison of benchmarks and performance, you can visit the following articles:\n",
      "  - [Comparative Analysis of A6000 vs A100 GPUs](https://www.cudocompute.com/blog/a-comparative-analysis-of-a6000-vs-a100-gpus-pytorch-performance)\n",
      "  - [NVIDIA A100 Datasheet](https://resources.nvidia.com/en-us-gpu/nvidia-a100-datashee)\n",
      "  - [NVIDIA RTX A6000 Review](https://www.storagereview.com/review/nvidia-rtx-a6000-review)\n",
      "\n",
      "In summary, the choice between the NVIDIA A100 and RTX A6000 depends on the specific requirements of your workload—AI and deep learning versus graphics rendering and visualization.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input('User: ')\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye',  'q']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "        \n",
    "    for event in graph.stream({'messages': ('user', user_input)}):\n",
    "        for value in event.values():\n",
    "            print('Assistant: ', value['messages'][-1].content)\n",
    "        print('-' * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed5669-630d-4dd9-8985-08c103dba86e",
   "metadata": {},
   "source": [
    "##  Adding Memory to the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae5d62-1342-46e6-9188-99878eaabd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c9a57d3-9ae8-423e-8117-6e3af0977e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the API key form .env\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e071e131-c321-44c8-b1ac-8b8e546b193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7fb29ffea690>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "tools = [tool]\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {'messages': [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "graph_builder.add_node('chatbot', chatbot)\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'chatbot',\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('tools', 'chatbot')\n",
    "graph_builder.set_entry_point('chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b12114d-2b9c-44ec-90ef-be5a672a322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.memory import MemorySaver ### fix\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(':memory:')\n",
    "graph = graph_builder.compile(checkpointer=MemorySaver()) ### fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0c3a43d-888c-48c9-bcd4-241de3fd6685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a412e83-0312-472a-8954-c459c15295e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {'thread_id': '1'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1f7c680-afae-4407-9a46-8d498b07ffbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the lastest AI model developed by OpenAI?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_Gp3mvbuTsWS9atCt3xMAg3vz)\n",
      " Call ID: call_Gp3mvbuTsWS9atCt3xMAg3vz\n",
      "  Args:\n",
      "    query: latest AI model developed by OpenAI\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/\", \"content\": \"ChatGPT maker OpenAI has announced its next major product release: A generative AI model code-named Strawberry, officially called OpenAI o1. OpenAI o1 avoids some of the reasoning pitfalls that normally trip up generative AI models because it can effectively fact-check itself by spending more time considering all parts of a question. What makes o1 “feel” qualitatively different from other generative AI models is its ability to “think” before responding to queries, according to OpenAI. (That’s less impressive when you consider that Google DeepMind’s recent AI achieved a silver medal in an equivalent to the actual IMO contest.) OpenAI also says that o1 reached the 89th percentile of participants — better than DeepMind’s flagship system AlphaCode 2, for what it’s worth — in the online programming challenge rounds known as Codeforces.\"}, {\"url\": \"https://openai.com/o1/\", \"content\": \"Introducing OpenAI o1. We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and other updates. Try it in ChatGPT Plus Try it in the API.\"}, {\"url\": \"https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/\", \"content\": \"Why OpenAI’s new model is such a big deal | MIT Technology Review This week we're going to talk about OpenAI’s impressive new reasoning model, called o1. However, last week OpenAI released a new model called o1 (previously referred to under the code name “Strawberry” and, before that, Q*) that blows GPT-4o out of the water for this type of purpose. Unlike previous models that are well suited for language tasks like writing and editing, OpenAI o1 is focused on multistep “reasoning,” the type of process required for advanced mathematics, coding, or other STEM-based questions. The new model also won’t be most users’ first pick for more language-heavy tasks, where GPT-4o continues to be the better option, according to OpenAI’s user surveys.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The latest AI model developed by OpenAI is called **OpenAI o1**, which was previously referred to by the code name \"Strawberry.\" This model is designed to enhance reasoning capabilities by spending more time \"thinking\" before responding to queries, allowing it to effectively fact-check itself. OpenAI o1 is particularly focused on multistep reasoning tasks, making it suitable for advanced mathematics, coding, and other STEM-related questions.\n",
      "\n",
      "For more information, you can check the following sources:\n",
      "- [TechCrunch article](https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/)\n",
      "- [OpenAI official page](https://openai.com/o1/)\n",
      "- [MIT Technology Review article](https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/)\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is the lastest AI model developed by OpenAI?'\n",
    "\n",
    "# streaming the events. \n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f52f19f-e626-48da-a3a3-bd58f7113f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What about Google?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_UpFkdcZvWoQPHxs6OvvZAmeh)\n",
      " Call ID: call_UpFkdcZvWoQPHxs6OvvZAmeh\n",
      "  Args:\n",
      "    query: latest AI model developed by Google\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://blog.google/technology/ai/google-gemini-ai/\", \"content\": \"This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.\\n This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\\nLearn more about Gemini’s capabilities and see how it works.\\n Gemini unlocks new scientific insights\\nUnderstanding text, images, audio and more\\nGemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. AI\\nIntroducing Gemini: our largest and most capable AI model\\nDec 06, 2023\\nmin read\\nMaking AI more helpful for everyone\\nA note from Google and Alphabet CEO Sundar Pichai:\\n\"}, {\"url\": \"https://techcrunch.com/2024/09/10/what-is-google-gemini-ai/\", \"content\": \"Google’s trying to make waves with Gemini, its flagship suite of generative AI models, apps, and services. Gemini apps can accept images as well as voice commands and text — including files like PDFs and soon videos, either uploaded or imported from Google Drive — and generate images. Speaking of integrations, the Gemini apps on the web and mobile can tap into Google services via what Google calls “Gemini extensions.” Gemini today integrates with Google Drive, Gmail, and YouTube to respond to queries such as “Could you summarize my last three emails?” Later this year, Gemini will be able to take additional actions with Google Calendar, Keep, Tasks, YouTube Music and Utilities, the Android-exclusive apps that control on-device features like timers and alarms, media controls, the flashlight, volume, Wi-Fi, Bluetooth, and so on.\"}, {\"url\": \"https://blog.google/technology/ai/gemini-collection/\", \"content\": \"Introducing Gemini: our largest and most capable AI model\\nGemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.\\n Collection\\nLearn more about Gemini, our most capable AI model\\nToday we introduced Gemini, our largest and most capable AI model — and the next step on our journey toward making AI helpful for everyone. And as we continue to refine Gemini Ultra, including completing extensive trust and safety checks, we’ll make it available to select groups before opening it up broadly to developers and enterprise customers early next year.\\n And in the coming months, Gemini will power features in more of our products and services like Ads, Chrome and Duet AI.\\n Starting December 13, developers and enterprise customers will be able to access Gemini Pro via the Gemini API in Vertex AI or Google AI Studio, our free web-based developer tool.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The latest AI model developed by Google is called **Gemini**. This model represents Google's flagship suite of generative AI models, applications, and services. Gemini is designed to be multimodal, meaning it can understand and process various types of inputs, including text, images, audio, and more. This capability allows it to handle nuanced information and answer complex questions effectively.\n",
      "\n",
      "Key features of Gemini include:\n",
      "- **Multimodal Understanding**: Gemini can process and understand text, images, audio, and other formats simultaneously.\n",
      "- **Integration with Google Services**: It is designed to work seamlessly with Google services like Google Drive, Gmail, and YouTube, allowing it to perform tasks such as summarizing emails or managing calendar events.\n",
      "- **Different Model Sizes**: Gemini is available in multiple sizes, including Ultra, Pro, and Nano, to cater to various use cases.\n",
      "\n",
      "For more detailed information, you can visit the following sources:\n",
      "- [Google AI Blog](https://blog.google/technology/ai/google-gemini-ai/)\n",
      "- [TechCrunch article](https://techcrunch.com/2024/09/10/what-is-google-gemini-ai/)\n",
      "- [Google AI Collection page](https://blog.google/technology/ai/gemini-collection/)\n"
     ]
    }
   ],
   "source": [
    "# It has memory!\n",
    "prompt = 'What about Google?'\n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "040ba00d-c5aa-447d-9ce2-8ece056ff1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What did I ask you so far?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "So far, you have asked me the following questions:\n",
      "\n",
      "1. What is the latest AI model developed by OpenAI?\n",
      "2. What about Google?\n",
      "\n",
      "If you have more questions or topics you'd like to discuss, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What did I ask you so far?'\n",
    "\n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b382ec29-a6da-48d4-bfad-4d2e008efb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What did I ask you so far?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You haven't asked me anything yet in this conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# NEW THREAD => no memory\n",
    "config1 = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "prompt = 'What did I ask you so far?'\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", prompt)]}, config1, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5085fdbf-0c91-454c-82ba-a0d369c0e6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='What is the lastest AI model developed by OpenAI?', additional_kwargs={}, response_metadata={}, id='de7f95dd-7834-4278-af38-eb5f68977d9b'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Gp3mvbuTsWS9atCt3xMAg3vz', 'function': {'arguments': '{\"query\":\"latest AI model developed by OpenAI\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 91, 'total_tokens': 115, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-84c0aa34-f9f7-4288-97e9-dbc025601471-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'latest AI model developed by OpenAI'}, 'id': 'call_Gp3mvbuTsWS9atCt3xMAg3vz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 24, 'total_tokens': 115}), ToolMessage(content='[{\"url\": \"https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/\", \"content\": \"ChatGPT maker OpenAI has announced its next major product release: A generative AI model code-named Strawberry, officially called OpenAI o1. OpenAI o1 avoids some of the reasoning pitfalls that normally trip up generative AI models because it can effectively fact-check itself by spending more time considering all parts of a question. What makes o1 “feel” qualitatively different from other generative AI models is its ability to “think” before responding to queries, according to OpenAI. (That’s less impressive when you consider that Google DeepMind’s recent AI achieved a silver medal in an equivalent to the actual IMO contest.) OpenAI also says that o1 reached the 89th percentile of participants — better than DeepMind’s flagship system AlphaCode 2, for what it’s worth — in the online programming challenge rounds known as Codeforces.\"}, {\"url\": \"https://openai.com/o1/\", \"content\": \"Introducing OpenAI o1. We\\'ve developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and other updates. Try it in ChatGPT Plus Try it in the API.\"}, {\"url\": \"https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/\", \"content\": \"Why OpenAI’s new model is such a big deal | MIT Technology Review This week we\\'re going to talk about OpenAI’s impressive new reasoning model, called o1. However, last week OpenAI released a new model called o1 (previously referred to under the code name “Strawberry” and, before that, Q*) that blows GPT-4o out of the water for this type of purpose. Unlike previous models that are well suited for language tasks like writing and editing, OpenAI o1 is focused on multistep “reasoning,” the type of process required for advanced mathematics, coding, or other STEM-based questions. The new model also won’t be most users’ first pick for more language-heavy tasks, where GPT-4o continues to be the better option, according to OpenAI’s user surveys.\"}]', name='tavily_search_results_json', id='796780ed-a7e8-4f4d-bb1a-1554bf4e2570', tool_call_id='call_Gp3mvbuTsWS9atCt3xMAg3vz', artifact={'query': 'latest AI model developed by OpenAI', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'OpenAI unveils o1, a model that can fact-check itself', 'url': 'https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/', 'content': 'ChatGPT maker OpenAI has announced its next major product release: A generative AI model code-named Strawberry, officially called OpenAI o1. OpenAI o1 avoids some of the reasoning pitfalls that normally trip up generative AI models because it can effectively fact-check itself by spending more time considering all parts of a question. What makes o1 “feel” qualitatively different from other generative AI models is its ability to “think” before responding to queries, according to OpenAI. (That’s less impressive when you consider that Google DeepMind’s recent AI achieved a silver medal in an equivalent to the actual IMO contest.) OpenAI also says that o1 reached the 89th percentile of participants — better than DeepMind’s flagship system AlphaCode 2, for what it’s worth — in the online programming challenge rounds known as Codeforces.', 'score': 0.9968885, 'raw_content': None}, {'title': 'OpenAI o1 Hub | OpenAI', 'url': 'https://openai.com/o1/', 'content': \"Introducing OpenAI o1. We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and other updates. Try it in ChatGPT Plus Try it in the API.\", 'score': 0.99442345, 'raw_content': None}, {'title': \"Why OpenAI's new model is such a big deal - MIT Technology Review\", 'url': 'https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/', 'content': \"Why OpenAI’s new model is such a big deal | MIT Technology Review This week we're going to talk about OpenAI’s impressive new reasoning model, called o1. However, last week OpenAI released a new model called o1 (previously referred to under the code name “Strawberry” and, before that, Q*) that blows GPT-4o out of the water for this type of purpose. Unlike previous models that are well suited for language tasks like writing and editing, OpenAI o1 is focused on multistep “reasoning,” the type of process required for advanced mathematics, coding, or other STEM-based questions. The new model also won’t be most users’ first pick for more language-heavy tasks, where GPT-4o continues to be the better option, according to OpenAI’s user surveys.\", 'score': 0.98955137, 'raw_content': None}], 'response_time': 1.78}), AIMessage(content='The latest AI model developed by OpenAI is called **OpenAI o1**, which was previously referred to by the code name \"Strawberry.\" This model is designed to enhance reasoning capabilities by spending more time \"thinking\" before responding to queries, allowing it to effectively fact-check itself. OpenAI o1 is particularly focused on multistep reasoning tasks, making it suitable for advanced mathematics, coding, and other STEM-related questions.\\n\\nFor more information, you can check the following sources:\\n- [TechCrunch article](https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/)\\n- [OpenAI official page](https://openai.com/o1/)\\n- [MIT Technology Review article](https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 614, 'total_tokens': 801, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-5ae7d53b-d2f0-4359-8c5c-74c879e6a4d8-0', usage_metadata={'input_tokens': 614, 'output_tokens': 187, 'total_tokens': 801}), HumanMessage(content='What about Google?', additional_kwargs={}, response_metadata={}, id='d1b32b32-4da4-4ebf-ae70-caeed768135d'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UpFkdcZvWoQPHxs6OvvZAmeh', 'function': {'arguments': '{\"query\":\"latest AI model developed by Google\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 812, 'total_tokens': 835, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-43316e28-984d-4251-b168-a695df8fb863-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'latest AI model developed by Google'}, 'id': 'call_UpFkdcZvWoQPHxs6OvvZAmeh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 812, 'output_tokens': 23, 'total_tokens': 835}), ToolMessage(content='[{\"url\": \"https://blog.google/technology/ai/google-gemini-ai/\", \"content\": \"This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.\\\\n This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\\\\nLearn more about Gemini’s capabilities and see how it works.\\\\n Gemini unlocks new scientific insights\\\\nUnderstanding text, images, audio and more\\\\nGemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. AI\\\\nIntroducing Gemini: our largest and most capable AI model\\\\nDec 06, 2023\\\\nmin read\\\\nMaking AI more helpful for everyone\\\\nA note from Google and Alphabet CEO Sundar Pichai:\\\\n\"}, {\"url\": \"https://techcrunch.com/2024/09/10/what-is-google-gemini-ai/\", \"content\": \"Google’s trying to make waves with Gemini, its flagship suite of generative AI models, apps, and services. Gemini apps can accept images as well as voice commands and text — including files like PDFs and soon videos, either uploaded or imported from Google Drive — and generate images. Speaking of integrations, the Gemini apps on the web and mobile can tap into Google services via what Google calls “Gemini extensions.” Gemini today integrates with Google Drive, Gmail, and YouTube to respond to queries such as “Could you summarize my last three emails?” Later this year, Gemini will be able to take additional actions with Google Calendar, Keep, Tasks, YouTube Music and Utilities, the Android-exclusive apps that control on-device features like timers and alarms, media controls, the flashlight, volume, Wi-Fi, Bluetooth, and so on.\"}, {\"url\": \"https://blog.google/technology/ai/gemini-collection/\", \"content\": \"Introducing Gemini: our largest and most capable AI model\\\\nGemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.\\\\n Collection\\\\nLearn more about Gemini, our most capable AI model\\\\nToday we introduced Gemini, our largest and most capable AI model — and the next step on our journey toward making AI helpful for everyone. And as we continue to refine Gemini Ultra, including completing extensive trust and safety checks, we’ll make it available to select groups before opening it up broadly to developers and enterprise customers early next year.\\\\n And in the coming months, Gemini will power features in more of our products and services like Ads, Chrome and Duet AI.\\\\n Starting December 13, developers and enterprise customers will be able to access Gemini Pro via the Gemini API in Vertex AI or Google AI Studio, our free web-based developer tool.\"}]', name='tavily_search_results_json', id='b85af704-bde9-4f45-82a5-e7148c009564', tool_call_id='call_UpFkdcZvWoQPHxs6OvvZAmeh', artifact={'query': 'latest AI model developed by Google', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"Introducing Gemini: Google's most capable AI model yet\", 'url': 'https://blog.google/technology/ai/google-gemini-ai/', 'content': 'This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.\\n This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\\nLearn more about Gemini’s capabilities and see how it works.\\n Gemini unlocks new scientific insights\\nUnderstanding text, images, audio and more\\nGemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. AI\\nIntroducing Gemini: our largest and most capable AI model\\nDec 06, 2023\\nmin read\\nMaking AI more helpful for everyone\\nA note from Google and Alphabet CEO Sundar Pichai:\\n', 'score': 0.9755303, 'raw_content': None}, {'title': 'Google Gemini: Everything you need to know about the generative AI models', 'url': 'https://techcrunch.com/2024/09/10/what-is-google-gemini-ai/', 'content': 'Google’s trying to make waves with Gemini, its flagship suite of generative AI models, apps, and services. Gemini apps can accept images as well as voice commands and text — including files like PDFs and soon videos, either uploaded or imported from Google Drive — and generate images. Speaking of integrations, the Gemini apps on the web and mobile can tap into Google services via what Google calls “Gemini extensions.” Gemini today integrates with Google Drive, Gmail, and YouTube to respond to queries such as “Could you summarize my last three emails?” Later this year, Gemini will be able to take additional actions with Google Calendar, Keep, Tasks, YouTube Music and Utilities, the Android-exclusive apps that control on-device features like timers and alarms, media controls, the flashlight, volume, Wi-Fi, Bluetooth, and so on.', 'score': 0.9554856, 'raw_content': None}, {'title': \"Everything to know about Gemini, Google's new AI model\", 'url': 'https://blog.google/technology/ai/gemini-collection/', 'content': 'Introducing Gemini: our largest and most capable AI model\\nGemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.\\n Collection\\nLearn more about Gemini, our most capable AI model\\nToday we introduced Gemini, our largest and most capable AI model — and the next step on our journey toward making AI helpful for everyone. And as we continue to refine Gemini Ultra, including completing extensive trust and safety checks, we’ll make it available to select groups before opening it up broadly to developers and enterprise customers early next year.\\n And in the coming months, Gemini will power features in more of our products and services like Ads, Chrome and Duet AI.\\n Starting December 13, developers and enterprise customers will be able to access Gemini Pro via the Gemini API in Vertex AI or Google AI Studio, our free web-based developer tool.', 'score': 0.8828325, 'raw_content': None}], 'response_time': 2.13}), AIMessage(content=\"The latest AI model developed by Google is called **Gemini**. This model represents Google's flagship suite of generative AI models, applications, and services. Gemini is designed to be multimodal, meaning it can understand and process various types of inputs, including text, images, audio, and more. This capability allows it to handle nuanced information and answer complex questions effectively.\\n\\nKey features of Gemini include:\\n- **Multimodal Understanding**: Gemini can process and understand text, images, audio, and other formats simultaneously.\\n- **Integration with Google Services**: It is designed to work seamlessly with Google services like Google Drive, Gmail, and YouTube, allowing it to perform tasks such as summarizing emails or managing calendar events.\\n- **Different Model Sizes**: Gemini is available in multiple sizes, including Ultra, Pro, and Nano, to cater to various use cases.\\n\\nFor more detailed information, you can visit the following sources:\\n- [Google AI Blog](https://blog.google/technology/ai/google-gemini-ai/)\\n- [TechCrunch article](https://techcrunch.com/2024/09/10/what-is-google-gemini-ai/)\\n- [Google AI Collection page](https://blog.google/technology/ai/gemini-collection/)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 257, 'prompt_tokens': 1538, 'total_tokens': 1795, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-6483f6ca-9bb9-4f68-83ff-0a2e24a59f8d-0', usage_metadata={'input_tokens': 1538, 'output_tokens': 257, 'total_tokens': 1795})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef79ac2-bb1b-609d-8008-4cafdc427cd4'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content=\"The latest AI model developed by Google is called **Gemini**. This model represents Google's flagship suite of generative AI models, applications, and services. Gemini is designed to be multimodal, meaning it can understand and process various types of inputs, including text, images, audio, and more. This capability allows it to handle nuanced information and answer complex questions effectively.\\n\\nKey features of Gemini include:\\n- **Multimodal Understanding**: Gemini can process and understand text, images, audio, and other formats simultaneously.\\n- **Integration with Google Services**: It is designed to work seamlessly with Google services like Google Drive, Gmail, and YouTube, allowing it to perform tasks such as summarizing emails or managing calendar events.\\n- **Different Model Sizes**: Gemini is available in multiple sizes, including Ultra, Pro, and Nano, to cater to various use cases.\\n\\nFor more detailed information, you can visit the following sources:\\n- [Google AI Blog](https://blog.google/technology/ai/google-gemini-ai/)\\n- [TechCrunch article](https://techcrunch.com/2024/09/10/what-is-google-gemini-ai/)\\n- [Google AI Collection page](https://blog.google/technology/ai/gemini-collection/)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 257, 'prompt_tokens': 1538, 'total_tokens': 1795, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-6483f6ca-9bb9-4f68-83ff-0a2e24a59f8d-0', usage_metadata={'input_tokens': 1538, 'output_tokens': 257, 'total_tokens': 1795})]}}, 'step': 8, 'parents': {}}, created_at='2024-09-23T13:03:04.617571+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef79ac2-9f0b-619e-8007-bceb5c019dfe'}}, tasks=())"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "959688de-a791-41fb-bb10-fea8309d5bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcddeb4-3e52-4ee3-bc3b-1ecb5c5a2fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
